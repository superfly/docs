---
title: Fly Proxy
layout: docs
nav: firecracker
---

Fly Proxy is the omnipresent routing layer between the public Internet and the Fly.io platform and within the global WireGuard mesh that encompasses everything that runs on Fly.io. Every Fly.io edge and worker runs Fly Proxy to do the heavy lifting of moving data around our infrastructure: managing traffic and load balancing, applying protocol handlers, and managing connections to services.

A summary of things Fly Proxy does:

- accept client connections and match them to your app's Machines in the closest region
- handle backhaul between Fly.io servers over WireGuard tunnels
- route requests to private Flycast addresses in your 6PN (private network)
- handle connections for different protocols with connection handler options:
  - terminate TLS by default for most public apps
  - terminate TLS for Postgres connections
  - accept PROXY protocol headers on incoming requests
- load balance requests to your app's Machines
- autostart/autostop Machines based on traffic to your app
- dynamic request routing with the `fly-replay` response header

## How Fly Proxy routes incoming public requests

The following description is about as high-level as it gets, but it's a start.

When one of your app's users makes a request to your app on Fly.io, the request lands on our closest edge server. Fly Proxy (on the edge server) checks out the details of the request, adds the appropriate headers, redirects HTTP traffic to HTTPS, and routes the request through the WireGuard tunnel backhaul to the nearest healthy worker server that hosts your app's Machines. Fly Proxy (on the worker this time) sends the request to a Machine running your app over a local virtual interface, so your app can do its thing. Fly Proxy handles the response back from the Machine to the worker, from the worker to the edge server, and back to the user.

Fly Proxy gets all its information about apps and Machines and services from corrosion, our service catalog that stores the state of pretty much everything on the Fly.io platform. Corrosion provides gossip-based service discovery that's highly-distributed, covering all our edges and regions.

## How Fly Proxy routes requests over your private network (6PN)

Apps can communicate with each other by default on your [private network](/docs/networking/private-networking/) without Fly Proxy's help, but if you want to use Fly Proxy features, then you can do that with [Flycast](/docs/networking/private-networking/#flycast-private-fly-proxy-services).

When you assign a Flycast address to your app, the traffic gets routed through the Fly Proxy while remaining entirely in your private network. When App 1 makes a request to App 2 in the same private network, Fly Proxy on the worker hosting App 1's Machines checks out the details of the request, adds the appropriate headers, and routes the request through the same WireGuard tunnel backhaul to the nearest healthy worker that hosts Machines for App 2. Fly Proxy on that server sends the request to a Machine running App 2.

## Connection handlers

Connection handler settings tell Fly Proxy how to convert TCP requests for specific protocols before they reach your app. Most standard web services use both `http` and `tls` handlers by default and there's an optional handler for apps that use PROXY protocol. Learn more about [connection handlers](/docs/networking/services/#connection-handlers). Postgres connections get their own `pg_tls` handler.

Handlers aren't mandatory; if you don't use them, then Fly Proxy just forwards TCP to your app with no changes.

## Load balancing

Fly Proxy routes requests to individual Machines in your app using a combination of concurrency settings in your app config, current load, and closeness.

### Load balancing strategy

The basic load balancing strategy is:

* Send traffic to the least loaded, closest Machine
* If multiple Machines have identical load and closeness, randomly choose one

#### Load

Fly Proxy determines load using the [concurrency settings](/docs/reference/configuration#services-concurrency) configured for an app and the current traffic relative to those settings.

The table below describes how traffic may or may not be routed to a Machine based on configured `soft_limit` and `hard_limit` values.

| Machine load | What happens |
|---|---|
| Above `hard_limit` | No new traffic will be sent to the Machine |
| At or above `soft_limit`, below `hard_limit` | Traffic will only be sent to this Machine if all other Machines are also above their `soft_limit` |
| Below `soft_limit` | Traffic will be sent to the Machine when it is the closest Machine that is under `soft_limit` |

#### Closeness

Closeness is determined by RTT (round-trip time) between the Fly.io edge server receiving a connection or request, and the worker server where your Machine runs. Even within the same region, we use different datacenters with different RTTs. These RTTs are measured constantly between all servers.

You can observe live RTT values between Fly.io regions using our [RTT app](https://rtt.fly.dev/).

### Example of load balancing for a web service

We have a hypothetical web service that we know can handle 25 concurrent requests with the configured CPU and memory settings. We set the following values in our fly.toml:

```toml
  [services.concurrency]
    type = "requests"
    hard_limit = 25
    soft_limit = 20
```

We set `type = "requests"` so Fly.io will use concurrent HTTP requests to determine when to adjust load. We prefer this to `type = "connections"`, because our web service does work for each request and our users may make multiple requests over a single connection (e.g., with HTTP/2). 

We set the `soft_limit` to 20, so we have a little room for Fly Proxy to shift load to other Machines before a single Machine becomes overwhelmed.

We deploy 10 Machines in four regions: `ams` (Amsterdam), `bom` (Mumbai), `sea` (Seattle), and `sin` (Singapore), with three of those in `ams`.

In this contrived example, all of the users are currently in Amsterdam, so the traffic is arriving at one of the Fly.io edges in Amsterdam. Here's what happens as the number of concurrent HTTP requests from users in Amsterdam increases:

- 60 concurrent requests: Requests are divided evenly between the 3 Machines in the `ams` region. Closeness of the worker and edge will determine which of the 3 Machines each request goes to.
- 61+ concurrent requests: 60 of those requests will be sent to the 3 Machines in the `ams` region and the rest will be sent to the closest Machines in other regions.
- 200+ concurrent requests: All Machines are at their `soft_limit`, so Fly Proxy will start routing requests to the `ams` Machines again. For example, the 201st concurrent request will go to a Machine in the `ams` region that is currently at its `soft_limit`.
- 250 concurrent requests: All Machines are at their `hard_limit`. The 251st concurrent request will get queued by Fly Proxy until a Machine is below its `hard_limit`.

If traffic is far above the `hard_limit` for a long period of time, Fly Proxy might start returning 503 Service Unavailable responses for requests that are not able to be routed to a Machine.

## Autostart/Autostop Machines

Fly Proxy can start and stop existing Machines based on incoming requests, so that your app can accommodate bursts in demand without keeping extra Machines running constantly.

Fly Proxy uses the same concurrency settings for autostart/autostop as for load balancing to determine when Machines have excess capacity and can be stopped.

Learn more about [how autostart/autostop works and how to configure it](/docs/apps/autostart-stop/). 

## Dynamic request routing with `fly-replay`

The `fly-replay` response header instructs the Fly proxy to redeliver (replay) the original request to another region or Machine in your app, or even another app in your organization. Your app needs to send a response with the `fly-replay` header specifying the region, Machine, or app and Fly Proxy replays the whole request as instructed.

Learn more about [dynamic request routing](https://fly.io/docs/networking/dynamic-request-routing/).
