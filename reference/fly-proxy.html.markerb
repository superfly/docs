---
title: Fly Proxy
layout: docs
nav: firecracker
---

The Fly Proxy is the routing layer between the public Internet and the Fly.io platform. Every Fly.io server, both edges and workers, runs Fly Proxy to do all the heavy lifting to move data around our infrastructure: managing traffic and load balancing, applying handlers, and managing connections to services.

Things Fly Proxy does:
- accept client connections and match them to your app's Machines in the closest region
- apply [connection handlers](https://fly.io/docs/networking/services/#connection-handlers) beyond regular HTTP, like TLS termination and PROXY protocol
- load balance requests to your app's Machines
- handle backhaul between Fly.io servers over WireGuard tunnels
- route requests to private Flycast addresses in your 6PN (private network)
- autostart/autostop Machines based on traffic to your app
- dynamic request routing with the `fly-replay` response header

## How Fly Proxy routes incoming public requests

When one of your app's users makes a request to your app on Fly.io, Fly Proxy routes that request to the closest edge server. Fly Proxy (on the edge server) checks out the details of the request, adds the appropriate headers, redirects HTTP traffic to HTTPS and routes the request through the WireGuard tunnel backhaul to the nearest healthy server in a region that hosts your app's Machines. Fly Proxy on the server (worker) sends the request to a Machine running your app, so your app can do its thing.

Fly Proxy gets all its information about apps and Machines and services from corrosion, our service catalog that stores the state of pretty much everything on the Fly.io platform. Corrosion provides gossip-based service discovery that's highly-distributed, covering all our edges and regions.

## How Fly Proxy routes requests over your private network (6PN)

TBD

## Connection handlers

TBD

## Load balancing

Fly Proxy routes requests to individual Machines in your app using a combination of concurrency settings specified on your application, current load, and closeness.

### Load balancing strategy

Our load balancing strategy is:
* Send traffic to the least loaded, closest Machine
* If multiple Machines have identical load and closeness, randomly choose one

#### Load

Load is determined by the [concurrency limits](/docs/reference/configuration#services-concurrency) configured for an application and the current traffic relative to those configured limits.

The table below describes how traffic may or may not be routed to an instance based on configured `soft_limit` and `hard_limit` values.

| Instance load | What happens |
|---|---|
| Above `hard_limit` | No new traffic will be sent to instance |
| At or above `soft_limit`, below `hard_limit` | Traffic will only be sent to this instance if all other instances are also above their `soft_limit` |
| Below `soft_limit` | Traffic will be sent to instance when it is closest instance that is under `soft_limit` |

#### Closeness

Closeness is determined by RTT (round-trip time) between the Fly.io edge node receiving a connection or request, and the worker node where your instance runs. Even within the same region, we use different datacenters with different RTTs. These RTTs are measured constantly between all servers.

*Note:* These values are not visible to Fly.io users. We share this information here to help clarify how load balancing decisions are made.

### Example load balancing for a web service

We have a hypothetical web service that we know can handle 25 concurrent requests with the currently configured CPU and memory settings. So, we set the following values in our fly.toml:

```toml
  [services.concurrency]
    type = "requests"
    hard_limit = 25
    soft_limit = 20
```

We set `type = "requests"` so Fly.io will use concurrent HTTP requests to determine when to adjust load. We prefer this to `type = "connections"`, because our web service does work for each request and our users may make multiple requests over a single connection (e.g., with HTTP/2).

We choose to set our `soft_limit` to 20, so we have a little room for Fly Proxy to shift load to other instances before a single instance becomes overwhelmed.

We deployed 10 Machines in four regions: `ams`, `bom`, `sea`, and `sin`, with three of those in `ams`.

In this contrived example, all of the users are currently in Amsterdam (ams region) so their traffic is arriving at one of the Fly.io edges in Amsterdam. There are currently 3 instances of our web service running in Amsterdam.

When users in Amsterdam generate up to 60 concurrent HTTP requests, those requests are divided evenly between the three application instances in the ams region. Closeness of the worker and edge will determine which of the 3 instances each request goes to.

When users generate 61+ concurrent requests from Amsterdam, 60 of those requests will be sent to the 3 instances in the ams region and then the rest will be sent to instances in other regions based on which ones are closest.

That keeps going until we get to 200+ concurrent requests. At 200 concurrent requests all application instances are at their `soft_limit`, so Fly.io will start routing requests to the ams instances again. E.g., the 201st concurrent request will go to an application instance in the ams region that is currently at its `soft_limit`.

When users generate 250 concurrent requests, all instances will be at their `hard_limit`. The 251st concurrent request will get queued by the Fly Proxy until an instance is below its `hard_limit`.

If traffic is far above the `hard_limit` for a long period of time, Fly.io may start returning 503 Service Unavailable responses for requests that are not able to be routed to an instance.
